{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Admissions Essays: Unsupervised Approaches using scikit-learn\n",
    "\n",
    "This notbook is designed to analyze every admissions essay submitted to Berkeley in the 2014-2015 academic year using Topic Modeling in Pythons scikit-learn package.  It begins with two CSV files (UTF-8) each containing unique and correspondeing dummie ID's for each of the two essay prompts that year.  There are column headers.\n",
    "\n",
    "### Personal Statement 1\n",
    "1. Freshmen: Describe the world you come from — for example, your family, community or school — and tell us how your world has shaped your dreams and aspirations.\n",
    "1. Transfers: What is your intended major? Discuss how your interest in the subject developed and describe any experience you have had in the field — such as volunteer work, internships and employment, participation in student organizations and activities — and what you have gained from your involvement.\n",
    "\n",
    "### Personal Statement 2\n",
    "1. Tell us about a personal quality, talent, accomplishment, contribution or experience that is important to you. What about this quality or accomplishment makes you proud, and how does it relate to the person you are?\n",
    "\n",
    "### Outline\n",
    "1. [Setup the Analysis](#0.-Setup-the-Analysis)\n",
    "  1. [Import Packages](#Import-Packages)\n",
    "  1. [Important Questions](#Important-Questions)\n",
    "  1. [Initialize Variables](#Initilize-Variables)\n",
    "1. [Import and view the data using Pandas](#1.-Import-and-view-the-data-using-Pandas)\n",
    "  1. [Import the data into a Pandas Dataframe](#Import-the-data-into-a-Pandas-Dataframe)\n",
    "  1. [Lable the Columns](#Lable-the-Columns)\n",
    "  1. Merge the dataframes\n",
    "  1. Review the Data\n",
    "1. [Explore the Data & Drop missing values](#2.-Explore-the-Data-using-Pandas)\n",
    "  1. Are the ID's Unique?\n",
    "  2. Find Missing Data\n",
    "  1. Drop Missing Data\n",
    "1. [Pre-Processing the Essays](#3.-Pre-Processing-the-Essays)\n",
    "  1. Cleaning the text and tokenizing\n",
    "  1. Remove Stopwords\n",
    "  1. Stem the Tokens\n",
    "1. [Creating a sample for testing](#4.-Creating-a-sample-for-testing)\n",
    "1. [Creating the DTM: scikit-learn](#5.-Creating-the-DTM:-scikit-learn)\n",
    "  1. CountVectorizer function\n",
    "1. [Tf-idf scores](#6.-Tf-idf-scores)\n",
    "  1. TfidfVectorizer function\n",
    "1. [Uncovering patterns using LDA](#7.-Uncovering-Patterns:-LDA)\n",
    "1. [The End!](#8.-The-End!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup the Analysis\n",
    "\n",
    "Before we get started we must import a number of packages we'll need, answer a few important questions, and set up a few variables we'll need later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "import datetime\n",
    "import platform\n",
    "import ast # This is used below to asign the variables on the fly.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#import socket # I think this was to get the computer name, but may be changed now that platform is being used. see socket.gethostname()\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "print('All packages successfully imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QUESTIONS TO ANSWER \n",
    "sample_yes_no = 'No'\n",
    "number_of_topics = 5 # to revert to original, remove 'number_of_topics' from 'n_topics'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initializes a few variables we'll need, and deals with a few of the answers to the questions we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_yes_no = sample_yes_no.upper()\n",
    "print('Analyse only a sample? '+sample_yes_no)\n",
    "\n",
    "#This establishes the start time to calculate the time it takes to run the script\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "\n",
    "#Using platform package to access the machine name.\n",
    "run_on = platform.node()\n",
    "\n",
    "print(run_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and view the data using Pandas\n",
    "\n",
    "First, we read our corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe. \n",
    "\n",
    "Note: Pandas is great for data munging and basic calculations because it's so easy to use, and its data structure is really intuitive. It's not memory efficient however, so you might quickly need to move away from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import a few packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets some background variables we'll need for later calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section automatically detects where the script is running and adjusts file paths accordingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (run_on == 'BensMBP') or (run_on == 'BensMBP.local'):\n",
    "    df1 = pandas.read_csv(\"/Volumes/Extra Space/Google Drive/Scholarship/Writing Projects - Personal/Admissions Essays/Data/PS1_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "    df2 = pandas.read_csv(\"/Volumes/Extra Space/Google Drive/Scholarship/Writing Projects - Personal/Admissions Essays/Data/PS2_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "    print('The script is running locally.')\n",
    "elif run_on == 'mercury':\n",
    "    df1 = pandas.read_csv(\"../data/originals/PS1_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "    df2 = pandas.read_csv(\"../data/originals/PS2_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "    print('The script is running on mercury.')\n",
    "else:\n",
    "    print('The file path is unclear on this machine.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can import the data from the CSV Pandas and begin our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create dataframes called \"df1\" and \"df2\n",
    "\n",
    "#df1 = pandas.read_csv(\"/Volumes/Extra Space/Google Drive/Scholarship/Writing Projects - Personal/Admissions Essays/Data/PS1_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "#df2 = pandas.read_csv(\"/Volumes/Extra Space/Google Drive/Scholarship/Writing Projects - Personal/Admissions Essays/Data/PS2_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "\n",
    "#for the server\n",
    "#df1 = pandas.read_csv(\"../data/originals/PS1_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "#df2 = pandas.read_csv(\"../data/originals/PS2_F16.csv\", sep = ',', encoding = 'utf_8')\n",
    "\n",
    "\n",
    "# View the dataframe.\n",
    "# Notice the metadata. The column \"Personal Statement 1 (RETIRED)\" contains our text of interest.\n",
    "# You can move the hashtag to view the other dataframe.\n",
    "df1\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable the Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can rename the colum headers so they are easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This renames the colum headers\n",
    "df1.columns = ['CPID', 'College', 'PS1']\n",
    "df2.columns = ['CPID', 'College', 'PS2']\n",
    "\n",
    "# View the dataframe.  You can move the hashtag to view the other dataframe.\n",
    "df1\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge the two dataframes on thier two common elements (CPID and College) using `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the two data frames so that we have one data frame with both questions attached to common CPID's and College.\n",
    "df = pandas.merge(df1, df2, on=['CPID', 'College'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to see how much memory is being used by this new dataframe.  We can do that with the `info` option.  We can also view individual essays, housed in particular cells, in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the amount of memory being occupied by this newly created element.  \n",
    "\n",
    "# PROGRESS\n",
    "print('Full oringinal dataframe created.')\n",
    "print(df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to review data data that is contained in the new dataframe we created.  This code looks at an essay in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print the first essay from the column 'PS1' the print file is more faithful to our data\n",
    "print(df['PS1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Data using Pandas\n",
    "\n",
    "Let's first evaluate the general nature of the data to see if the ID's are unique, if there is any missing data, etc.  \n",
    "We can also look at some descriptive statistics about this data set to get a feel for what's in it. We'll do this using the Pandas package. \n",
    "\n",
    "Note: this is always good practice. It serves two purposes. It checks to make sure your data is correct, and there's no major errors. It also keeps you in touch with your data, which will help with interpretation. Love your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the ID's Unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ID's have more than one \"PS1\"s can be found by counting and ranking \"ID\"s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This tells us if we have any duplicate IDs.  If each response is 1 we are ok.\n",
    "print(df['CPID'].value_counts())\n",
    "\n",
    "# This code seems to check for duplicate CPIDs.  If it's blank there are no duplicates.\n",
    "print()\n",
    "print('Array containing duplicate CPIDs:')\n",
    "print(df.set_index('CPID').index.get_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced opperations will not work with empty data.  The next few steps are designed to find, exlpore and purge records with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This creates a variable empties\n",
    "\n",
    "# First for PS1\n",
    "print('Summarizing missing data for PS1:')\n",
    "empties_PS1 = numpy.where(pandas.isnull(df['PS1']))[0]\n",
    "\n",
    "print(empties_PS1)\n",
    "\n",
    "# you notice that this is not formatted as a list.  The next opperation \"list\" gets it in the right format.\n",
    "empties_PS1 = list(empties_PS1)\n",
    "print(empties_PS1)\n",
    "\n",
    "#This counts the number of missing essays for PS1.\n",
    "print(len(empties_PS1))\n",
    "\n",
    "#This lists the elemtns with missing data.\n",
    "df.iloc[empties_PS1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the above steps for PS2\n",
    "print('Summarizing missing data for PS2')\n",
    "empties_PS2 = numpy.where(pandas.isnull(df['PS2']))[0]\n",
    "print(empties_PS2)\n",
    "empties_PS2 = list(empties_PS2)\n",
    "print(empties_PS2)\n",
    "print(len(empties_PS2))\n",
    "df.iloc[empties_PS2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can create a list of every ID which has at least one missing essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This combines the two lists of missing data without duplicateing anything.\n",
    "empties_any = empties_PS1 + list(set(empties_PS2) - set(empties_PS1))\n",
    "empties_any.sort()\n",
    "\n",
    "print(empties_any)\n",
    "print('There are', len(empties_any), 'CPIDs with at least one missing essay.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the list of ID's that have at least one missing essay and drops them, creating a new dataframe where each cell ID populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_missing = df.drop(df.index[empties_any])\n",
    "\n",
    "# PROGRESS\n",
    "print('Records with missing data have beend dropped.')\n",
    "\n",
    "# df_no_missing = df.dropna()\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-Processing the Essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a Pandas Dataframe in the appropreiate structure, we can begin to process the text in the two essay columns.  This invovles cleaning the text in a number of ways before tokenizing the essays.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text and tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below combines multiple preprocessing steps into a singl eline of code.  It is repeated twice for each of the essays, and results in a largely \"preprocessed\", tokenized new column.  most of this is accomplished with the \"str\" feature of python.  Here is what we accomplished with each step:\n",
    "1. `str.replace('\\\\', ' ')` - This removes some of the ideosyncratic backslashes that were present \n",
    "1. `str.lower()` - this shifts all the letters to lowercase\n",
    "1. `str.replace('[^\\w\\s]','')` - This gets rid of punctuation.  The \"`^`\" is a negated set, the \"`\\w`\" matches any word character (alphanumeric & underscore), and the \"`\\s`\" matches any whitespace character (spaces, tabs, line breaks).\n",
    "1. `str.replace('[\\d]','')` - This gets rid of all numbers.\n",
    "1. `str.split()` - This tokenizes whats left, creating a list within the pandas cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create two new columns with tokenized essay responses.  \n",
    "#In the same opperation it make everything lowercase.\n",
    "df_no_missing['PS1_clean'] = df_no_missing['PS1'].str.replace('\\\\', ' ').str.lower().str.replace('[^\\w\\s]','').str.replace('[\\d]','').str.split()\n",
    "df_no_missing['PS2_clean'] = df_no_missing['PS2'].str.replace('\\\\', ' ').str.lower().str.replace('[^\\w\\s]','').str.replace('[\\d]','').str.split()\n",
    "\n",
    "# PROGRESS\n",
    "print('Step 1 of preprocessing complete.')\n",
    "\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this shows that we've mostly delt with the odd backslashes and cleand the text in a bunch of other ways!\n",
    "print(df_no_missing['PS1_clean'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are  words that appear frequently and tend not to be distinctive.  They are generally removed prior to text analysis unless there is compelling reason to keep them. [More info](http://www.nltk.org/book/ch02.html#code-unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stopwords imported from NLTK Above\n",
    "\n",
    "#Removes english stop words from the tokenized columns\n",
    "stop_words = stopwords.words('english')\n",
    "df_no_missing['PS1_clean'] = df_no_missing['PS1_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df_no_missing['PS2_clean'] = df_no_missing['PS2_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# PROGRESS\n",
    "print('Stopwords have been removed.')\n",
    "\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem the Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming reduces words with multiple endings to thier common stem.  There are multiple ways to do this, but we will use the Porter Stemmer for our purpouses. http://www.bogotobogo.com/python/NLTK/Stemming_NLTK.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using the \"Porter Stemmer\" we'll stem the words\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# There is lots of code in the saved file to rework this if nessisary.\n",
    "\n",
    "#This works on my machine with NLTK 3.2.1, but not on Mercury when it had NLTK 3.2.2!\n",
    "df_no_missing['PS1_clean'] = df_no_missing['PS1_clean'].apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "df_no_missing['PS2_clean'] = df_no_missing['PS2_clean'].apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "# PROGRESS\n",
    "print('Stemmed the tokens.')\n",
    "\n",
    "'''\n",
    "print('Test List 1')\n",
    "print(test_list_1)\n",
    "print('Test List 2')\n",
    "print(test_list_2)\n",
    "'''\n",
    "\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the tokens back to a string so we can execute count vectorizer and create a documnet term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_missing['PS1_clean'] = df_no_missing['PS1_clean'].apply(lambda x: ' '.join(x)) # for item in x])\n",
    "df_no_missing['PS2_clean'] = df_no_missing['PS2_clean'].apply(lambda x: ' '.join(x)) # for item in x])\n",
    "\n",
    "# PROGRESS\n",
    "print('Transformed the tokens back into strings.')\n",
    "\n",
    "# text_list_stemmed = [' '.join([porter_stemmer.stem(word) for word in sentence.split(\" \")]) for sentence in text_list]\n",
    "df_no_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a sample for testing\n",
    "\n",
    "In this section we'll create a smaller sample of the code to that the analysis we construct below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This generates a random sample of N essays, with a random state set for reproducability.\n",
    "# N can be slowly increased slowly to test the computational resources required as you scale up\n",
    "df_sample = df_no_missing.sample(n=500, random_state=0)\n",
    "\n",
    "# This code resets the indexs so that sorted orininals are kept and new ones are generated.\n",
    "df_sample = df_sample.sort_index()\n",
    "df_sample = df_sample.reset_index()\n",
    "\n",
    "# PROGRESS\n",
    "print('Created a sample of the data.')\n",
    "\n",
    "#This is where I assign the sample data to be analyzed.  If I want to run the whole dataset, comment this out.\n",
    "#df_no_missing = df_sample\n",
    "\n",
    "if (sample_yes_no == 'Y') or (sample_yes_no == 'YES'):\n",
    "    df_no_missing = df_sample\n",
    "    print('Running on the sample, not the whole dataset.')\n",
    "elif (sample_yes_no == 'no') or (sample_yes_no == 'No') or (sample_yes_no == 'NO') or (sample_yes_no == 'n') or (sample_yes_no == 'N'):\n",
    "    print('Running on the entire dataset.')\n",
    "else:\n",
    "    print('Unable to determine what data to analyze, the sample or the entire set.')\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the DTM: scikit-learn\n",
    "\n",
    "Now that we've preprocessed the text and created two colums with strings, the required imput for scikit-learn's CountVectorizer, we can create a documnet term matrix.  This is the building block for Topic Modeling and a number of other methods we may want to explore.  There are two ways to do this. We can turn it into a sparse matrix type, which can be used within scikit-learn for further analyses.  We can then turn it into a full documnet term matrix, but this is very memory intensive and might not be a great idea for larger data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see above for: from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "#Original sklearn_dtm = CountVectorizer().fit_transform(df.PS1)\n",
    "#I added the '.values.astype('U')' for an interim step in the section below. \n",
    "#It seemed to fix the count vectorizer issues\n",
    "sklearn_dtm_PS1 = countvec.fit_transform(df_no_missing['PS1_clean'])\n",
    "sklearn_dtm_PS2 = countvec.fit_transform(df_no_missing['PS2_clean'])\n",
    "\n",
    "print('PS1 sparse matrix type')\n",
    "print(sklearn_dtm_PS1)\n",
    "print(' ')\n",
    "print('PS2 sparse matrix type')\n",
    "print(sklearn_dtm_PS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tf-idf scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. We saw a few ways to this yesterday, using natural language processing. Today, we'll learn one simple approach to this: word scores. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is *tf-idf* scores. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "More precisely, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "You can, and often should, normalize the numerator: \n",
    "\n",
    "tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. We'll use it, but a challenge for you: use Pandas to calculate this manually. \n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see above for from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvec = TfidfVectorizer()\n",
    "\n",
    "# #create the dtm, but with cells weigthed by the tf-idf score.\n",
    "# dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.PS1).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "# #view results\n",
    "# dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(dtm_tfidf_df.max().sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uncovering Patterns: LDA\n",
    "\n",
    "Frequency counts and tf-idf scores are done at the word level. There are other methods of exporatory or unsupervised analysis on the document level and by examining the co-occurrence of words within documents. Scikit-learn allows for many of these methods, including:\n",
    "\n",
    "* document clustering\n",
    "* document or word similarities using cosine similarity\n",
    "* pca\n",
    "* topic modeling\n",
    "\n",
    "We'll run through an example of topic modeling here. Again, the goal is not to learn everything you need to know about topic modeling. Instead, this will provide you some starter code to run a simple model, with the idea that you can use this base of knowledge to explore this further.\n",
    "\n",
    "We will run Latent Dirichlet Allocation, the most basic and the oldest version of topic modeling. We will run this in one big chunk of code. Our challenge: use our knowledge of scikit-learn that we gained aboe to walk through the code to understand what it is doing. Your challenge: figure out how to modify this code to work on your own data, and/or tweak the parameters to get better output.\n",
    "\n",
    "Note: we will be using a different dataset for this technique. The music reviews in the above dataset are often short, one word or one sentence reviews. Topic modeling is not really appropriate for texts that are this short. Instead, we want texts that are longer and are composed of multiple topics each. For this exercise we will use a database of children's literature from the 19th century. \n",
    "\n",
    "The data were compiled by students in this course: http://english197s2015.pbworks.com/w/page/93127947/FrontPage\n",
    "Found here: http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244469/Data%20Collections%20and%20Datasets#demo-corpora\n",
    "\n",
    "That page has additional corpora, for those interested in exploring text analysis further.\n",
    "\n",
    "I did some minimal cleaning to get the children's literature data in .csv format for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_lit = pandas.read_csv(\"/Volumes/Extra Space/Google Drive/Scholarship/Writing Projects - Personal/Admissions Essays/Small Sample/AdmissionsEssays/statement_test_031417.csv\", sep = ',', encoding = 'utf-8')\n",
    "\n",
    "# #drop rows where the text is missing. I think there's only one row where it's missing, but check me on that.\n",
    "# df_lit = df_lit.dropna(subset=['PS1'])\n",
    "\n",
    "#df_lit = df_no_missing\n",
    "\n",
    "#view the dataframe\n",
    "#df_lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit the model. This requires the use of CountVecorizer, which we've already used, and the scikit-learn function LatentDirichletAllocation.\n",
    "\n",
    "See [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for more information about this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#should switch to batch (from online).  n_samples should be closer to full set\n",
    "\n",
    "####Adopted From: \n",
    "#Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# See above for: from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# and:from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = number_of_topics #changed this so the value can be set above.\n",
    "n_top_words = 100\n",
    "\n",
    "##This is a function to print out the top words for each topic in a pretty way.\n",
    "#Don't worry too much about understanding every line of this code.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# Use tf-idf features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.80, min_df=50,\n",
    "                                   max_features=None,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_no_missing['PS1_clean'])\n",
    "\n",
    "# Use tf (raw term count) features\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                max_features=None,\n",
    "                                stop_words='english'\n",
    "                                )\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_no_missing['PS1_clean'])\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_topics=%d...\"\n",
    "      % (n_samples, n_topics))\n",
    "\n",
    "#define the lda function, with desired options TAKE A LOOK AT THIS.  MIGHT BE TOO FEW\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=100,  #100 (THATS WHAT LAURA DID)\n",
    "                                learning_method='batch',  # CHANGE THIS from 'online' TO 'batch'\n",
    "                                learning_offset=80.,\n",
    "                                total_samples=n_samples,\n",
    "                                random_state=0)\n",
    "#fit the model\n",
    "lda.fit(tf)\n",
    "\n",
    "#print the top words per topic, using the function defined above.\n",
    "#Unlike R, which has a built-in function to print top words, we have to write our own for scikit-learn\n",
    "#I think this demonstrates the different aims of the two packages: R is for social scientists, Python for computer scientists\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Topic Document Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we may want to do with the output is find the most representative texts for each topic. A simple way to do this (but not memory efficient), is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array. The numbers produced here sum to 1 for every row, and represent the relative representativeness of each topic.  The topic with the highest number is the most likely topic to have structured the documnemt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(tf)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a Panda's Dataframe and do some work to rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this reads the array into a dataframe\n",
    "topic_dist_df = pandas.DataFrame(topic_dist)\n",
    "\n",
    "#This is a little script to name the colum names appropriately\n",
    "col_number = n_topics\n",
    "col_nubber_original = col_number\n",
    "column_name_string_PS1 = \"\"\n",
    "for i in range(0,col_number):\n",
    "    column_name_string_PS1+=\"'Topic_\"+str(col_nubber_original-col_number)+\"_PS1', \"\n",
    "    col_number = col_number - 1\n",
    "\n",
    "\n",
    "#This cleans up the end of the string produced above and adds brackets\n",
    "column_name_string_PS1 = column_name_string_PS1[:-2]\n",
    "column_name_string_PS1 = '['+column_name_string_PS1+']'\n",
    "\n",
    "\n",
    "#This converts the string into a list type object.\n",
    "column_name_string_PS1 = ast.literal_eval(column_name_string_PS1)\n",
    "\n",
    "#This applies the string contining the names of the colums to the dataframe\n",
    "topic_dist_df.columns = column_name_string_PS1\n",
    "\n",
    "topic_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the new dataframe with the original dataframe of essays and ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_w_topics = df_no_missing.join(topic_dist_df)\n",
    "\n",
    "# PROGRESS\n",
    "print('Joined the topic and oringinal dataframes.')\n",
    "\n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Documnet Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Writing the output of this run to a CSV\n",
    "#df_w_topics.to_csv('Admissions_PS1_Full_'+time_for_f_name+'.csv', sep=',')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "time_for_f_name = now.strftime(\"Date_%Y-%m-%d_Time_%H-%M\")\n",
    "path = '../data/'\n",
    "topic_num_string = str(number_of_topics)\n",
    "\n",
    "#print(path+'Admissions_PS1_Full_'+time_for_f_name+'.csv')\n",
    "\n",
    "# add this   number_of_topics   and test it: \n",
    "df_w_topics.to_csv(path+'Admissions_PS1_'+topic_num_string+'_Topic_'+time_for_f_name+'.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Feature Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first few steps walk through the data that we have and tries to get it into a format that is readable.  First, we take a look at a list of words stored in tf_feautre_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(tf_feature_names))\n",
    "print(type(tf_feature_names))\n",
    "print(tf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need to transform the list into an array, which we can do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This transforms the list into a numpy array\n",
    "tf_feature_names_array = numpy.asarray(tf_feature_names)\n",
    "print(type(tf_feature_names_array))\n",
    "print(tf_feature_names_array.shape)\n",
    "print(tf_feature_names_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit of an asside, but we see that this matrix stores the topic weights for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These identical arrays provide the topic weighting for each text.\n",
    "print(lda.transform(tf))\n",
    "print(lda.transform(tf).shape)\n",
    "print(topic_dist)\n",
    "print(topic_dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a matrix that seems to store the feature weights.  We notice that the dimentions of this martix correspond to the number of topics and the dimentions of the word list we just reviewed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These two things tell us what is in the LDA model at this point\n",
    "#I believe this tell us the weights of the features, which in this case are words\n",
    "\n",
    "#this tells us the dimentions of the array\n",
    "print(lda.components_.shape)\n",
    "print(type(lda.components_))\n",
    "\n",
    "#This shows that components is an array of numbers\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transposes the matrix so it matches the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transposed_components = numpy.transpose(lda.components_)\n",
    "print(transposed_components.shape)\n",
    "print(type(transposed_components))\n",
    "print(transposed_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can conver the feature list into a Pandas DataFrame for easy viewing and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_new = pandas.DataFrame(tf_feature_names_array)\n",
    "item_new.columns = ['Feature_Words']\n",
    "item_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a Pandas DataFrame out of the feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_new_2 = pandas.DataFrame(transposed_components)\n",
    "item_new_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This names the feautre weight colums on the fly.  We can change the number of topics without upsetting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a little script to name the colum names appropriately\n",
    "col_number = n_topics\n",
    "col_nubber_original = col_number\n",
    "column_name_string_PS1_features = \"\"\n",
    "for i in range(0,col_number):\n",
    "    column_name_string_PS1_features+=\"'Topic_\"+str(col_nubber_original-col_number)+\"_PS1_features', \"\n",
    "    col_number = col_number - 1\n",
    "\n",
    "\n",
    "#This cleans up the end of the string produced above and adds brackets\n",
    "column_name_string_PS1_features = column_name_string_PS1_features[:-2]\n",
    "column_name_string_PS1_features = '['+column_name_string_PS1_features+']'\n",
    "\n",
    "\n",
    "#This converts the string into a list type object.\n",
    "column_name_string_PS1_features = ast.literal_eval(column_name_string_PS1_features)\n",
    "\n",
    "#This applies the string contining the names of the colums to the dataframe\n",
    "item_new_2.columns = column_name_string_PS1_features\n",
    "\n",
    "item_new_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the two data frames we saw above: the features (terms), and the feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_crazy = pandas.concat([item_new, item_new_2], axis=1)\n",
    "df_crazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This saves the feature matrix to a csv\n",
    "df_crazy.to_csv(path+'Admissions_PS1_Feature_Words_'+topic_num_string+'_Topic_'+time_for_f_name+'.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Final Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort the dataframe for the topic of interest, and view the top documents for the topics.\n",
    "Below we sort the documents first by Topic 0 (looking at the top words for this topic I think it's about family, health, and domestic activities), and next by Topic 1 (again looking at the top words I think this topic is about children playing outside in nature). These topics may be a family/nature split?\n",
    "\n",
    "Look at the titles for the two different topics. Look at the gender of the author. Hypotheses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read individual essays in full using the code below.  Change the number in the final set of brackets to point to a spesific serial number (ID-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_w_topics[['CPID', 'PS1', 'Topic_0_PS1']].sort_values(by=['Topic_0_PS1'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_w_topics['PS1'][205])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to sort the most imporant words/features for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_0_feature_terms = df_crazy.sort_values(['Topic_2_PS1_features'], ascending=False)\n",
    "topic_0_feature_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The End!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This calculates the amount of time it took to run the model, it requires \"start_time\" set at start.\n",
    "end_time = time.time()\n",
    "total_seconds = end_time - start_time\n",
    "\n",
    "m, s = divmod(total_seconds, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(\"This script took %d:%02d:%02d to run (h:m:s)\" % (h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Done! The output of the section above corresponds to Admissions_PS1_Full_'+time_for_f_name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
